{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182e7b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import json\n",
    "from PIL import Image\n",
    "from prettytable import PrettyTable\n",
    "from sklearn.utils import shuffle\n",
    "from scipy.signal import medfilt\n",
    "from skimage.segmentation import clear_border\n",
    "from skimage.measure import label, regionprops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddb4588",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation:\n",
    "    def relu(self, X):\n",
    "        \"\"\"\n",
    "        Memberikan nilai output berupa fungsi ReLU dari X\n",
    "        \n",
    "        Parameter:\n",
    "        X: Numpy 2D Array berukuran (jumlah fitur, jumlah data)\n",
    "        \n",
    "        Return:\n",
    "        Y: Numpy 2D Array yang merupakan output dari fungsi ReLU\n",
    "        \"\"\"\n",
    "        Y = np.maximum(X, 0)\n",
    "        return Y\n",
    "    \n",
    "    def softmax(self, X):\n",
    "        \"\"\"\n",
    "        Memberikan nilai output berupa fungsi Softmax dari X\n",
    "        \n",
    "        Parameter:\n",
    "        X: Numpy 2D Array berukuran (jumlah fitur, jumlah data)\n",
    "        \n",
    "        Return:\n",
    "        Y: Numpy 2D Array yang merupakan output dari fungsi Softmax\n",
    "        \"\"\"\n",
    "        # Menghindari overflow (https://www.techopedia.com/definition/663/overflow-error#:~:text=In%20computing%2C%20an%20overflow%20error,of%20its%20ability%20to%20handle.)\n",
    "        X -= np.max(X, axis=0, keepdims=True)\n",
    "        # Perhitungan\n",
    "        exp_X = np.exp(X)\n",
    "        sum_exp_X = np.sum(exp_X, axis=0, keepdims=True)\n",
    "        Y = exp_X / sum_exp_X\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4d0e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics:\n",
    "    def categorical_crossentropy(self, Y_pred, Y_true):\n",
    "        \"\"\"\n",
    "        Memberikan nilai output metrik categorical crossentropy dengan membandingkan Y_pred dengan Y_true\n",
    "        \n",
    "        Parameter:\n",
    "        Y_pred: Numpy 2D Array berukuran (jumlah fitur, jumlah data) yang merupakan hasil prediksi\n",
    "        Y_true: Numpy 2D Array berukuran (jumlah fitur, jumlah data) yang merupakan nilai target pada data\n",
    "        \n",
    "        Return:\n",
    "        cost: Nilai skalar yang merupakan hasil output dari categorical crossentropy\n",
    "        \"\"\"\n",
    "        # Perhitungan\n",
    "        cross_entropy = -np.sum(Y_true * np.log(Y_pred))\n",
    "        cost = cross_entropy / Y_true.shape[1]\n",
    "        return cost\n",
    "\n",
    "    def accuracy(self, Y_pred, Y_true):\n",
    "        \"\"\"\n",
    "        Memberikan nilai output metrik akurasi\n",
    "        \n",
    "        Parameter:\n",
    "        Y_pred: Numpy 2D Array berukuran (jumlah fitur, jumlah data) yang merupakan hasil prediksi\n",
    "        Y_true: Numpy 2D Array berukuran (jumlah fitur, jumlah data) yang merupakan nilai target pada data\n",
    "        \n",
    "        Return:\n",
    "        acc: Nilai skalar yang merupakan hasil output dari akurasi\n",
    "        \"\"\"\n",
    "        pred_matrix = Y_pred == np.max(Y_pred, axis=0)\n",
    "        acc = np.sum(pred_matrix * Y_true) / Y_pred.shape[1]\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dc929e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam:\n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        self.t = 0\n",
    "        \n",
    "    def update(self, w, dw):\n",
    "        if self.m is None:\n",
    "            self.m = np.zeros_like(w)\n",
    "            self.v = np.zeros_like(w)\n",
    "            \n",
    "        self.t += 1\n",
    "        self.m = self.beta1 * self.m + (1 - self.beta1) * dw\n",
    "        self.v = self.beta2 * self.v + (1 - self.beta2) * (dw**2)\n",
    "        mb = self.m / (1 - self.beta1**self.t)\n",
    "        vb = self.v / (1 - self.beta2**self.t)\n",
    "        w -= self.lr * mb / (np.sqrt(vb) + self.epsilon)\n",
    "        \n",
    "        return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9454efa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2D(Activation):\n",
    "    def __init__(self, filters, kernel_size, activation=\"relu\", input_shape=(None,)):\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.activation = activation\n",
    "        self.input_shape = (None,) + input_shape\n",
    "        self.alias = \"conv_2d\"\n",
    "        \n",
    "    def initialize_parameters(self, lr, beta1, beta2, epsilon):\n",
    "        # Ukuran Output\n",
    "        self.output_shape = (None, self.input_shape[1] - self.kernel_size[0] + 1, self.input_shape[2] - self.kernel_size[1] + 1,\n",
    "                             self.filters)\n",
    "        # Kernel\n",
    "        self.W = np.random.randn(self.filters, self.kernel_size[0], self.kernel_size[1], self.input_shape[-1]) * 0.001\n",
    "        self.adam_W = Adam(lr, beta1, beta2, epsilon)\n",
    "        # Bias\n",
    "        self.B = np.zeros((self.filters, 1))\n",
    "        self.adam_B = Adam(lr, beta1, beta2, epsilon)\n",
    "        # Jumlah Parameter\n",
    "        self.param_nums = np.prod(self.W.shape) + np.prod(self.B.shape)\n",
    "    \n",
    "    def convolution(self, X, W, B=np.array(0)):\n",
    "        \"\"\"\n",
    "        Mengembalikan matriks citra hasil konvolusi\n",
    "        \n",
    "        Parameter:\n",
    "        X: Matriks Input Numpy Array 4D berukuran (jumlah data, panjang pixel, lebar pixel, anchor)\n",
    "        W: Matriks Kernel Numpy Array 4D berukuran (panjang kernel, lebar kernel, anchor input, jumlah filter)\n",
    "        B: Vektor Bias (Default: 0)\n",
    "        \n",
    "        Return:\n",
    "        conv_images: Matriks Konvolusi Numpy Array 4D\n",
    "        \"\"\"\n",
    "        conv_images = np.zeros((X.shape[0], X.shape[1] - W.shape[1] + 1, \\\n",
    "                                     X.shape[2] - W.shape[2] + 1, W.shape[0]))\n",
    "        img_len, conv_x, conv_y, filters = conv_images.shape\n",
    "        for idx in range(img_len):\n",
    "            for i in range(conv_x):\n",
    "                for j in range(conv_y):\n",
    "                    sub_image = X[idx, i:i+W.shape[1], j:j+W.shape[2], :]\n",
    "                    conv_images[idx, i, j, :] = np.sum(sub_image * W, axis=(1, 2, 3)) + B.flatten()\n",
    "        return conv_images\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.Z = self.convolution(X, self.W, self.B)\n",
    "        if self.activation.lower() == \"relu\":\n",
    "            self.A = super().relu(self.Z)\n",
    "        return self.A\n",
    "    \n",
    "    def backpro(self, prev_layer, next_layer):\n",
    "        dC = np.zeros_like(self.A)\n",
    "        img_len, m, n, anchors = self.A.shape\n",
    "        \n",
    "        # Jika ukuran output adalah ganjil\n",
    "        if m % 2 == 1:\n",
    "            m -= 1\n",
    "        if n %2 == 1:\n",
    "            n -= 1\n",
    "        \n",
    "        for idx in range(img_len):\n",
    "            for i in range(0, m, 2):\n",
    "                for j in range(0, n, 2):\n",
    "                    sub_C = self.A[idx, i:i+2, j:j+2, :]\n",
    "                    dC[idx, i:i+2, j:j+2, :] = np.where(sub_C == np.max(sub_C, axis=(0, 1), keepdims=True),\n",
    "                                                        np.reshape(next_layer.dP[idx, int(i/2), int(j/2), :], (1, 1, -1)), 0) /\\\n",
    "                                               np.where(np.sum(sub_C == np.max(sub_C, axis=(0, 1), keepdims=True), axis=(0, 1), keepdims=True) > 1, 2, 1)\n",
    "        # dC/dZ\n",
    "        if self.activation.lower() == \"relu\":\n",
    "            dCdZ = np.where(self.Z > 0, 1, 0)\n",
    "        # dZ = dcost/dZ\n",
    "        self.dZ = dC * dCdZ\n",
    "        \n",
    "        if hasattr(prev_layer, \"A\"):\n",
    "            self.dW = self.convolution(prev_layer.A.T, self.dZ.T).T / img_len\n",
    "        else:\n",
    "            self.dW = self.convolution(prev_layer.T, self.dZ.T).T / img_len\n",
    "            \n",
    "        self.dB = np.expand_dims(np.sum(self.dZ, axis=(0, 1, 2)), axis=-1) / img_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75ba13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPooling(Conv2D):\n",
    "    def __init__(self, pool_size=(2, 2), input_shape=(None,)):\n",
    "        self.pool_size = pool_size\n",
    "        self.input_shape = (None,) + input_shape\n",
    "        self.alias = \"maxpool_2d\"\n",
    "        \n",
    "    def initialize_parameters(self, lr, beta1, beta2, epsilon):\n",
    "        # Ukuran Output\n",
    "        self.output_shape = (None, self.input_shape[1] // self.pool_size[0], \\\n",
    "                             self.input_shape[2] // self.pool_size[1], self.input_shape[-1])\n",
    "        # Banyak parameter\n",
    "        self.param_nums = 0\n",
    "        \n",
    "    def pooling(self, X, pool_size=(2, 2)):\n",
    "        \"\"\"\n",
    "        Mengembalikan matriks citra hasil max pooling\n",
    "        \n",
    "        Parameter:\n",
    "        X: Matriks Input Numpy Array 4D berukuran (jumlah data, panjang pixel, lebar pixel, anchor)\n",
    "        pool_size: Ukuran sub-matriks Numpy Array 2D yang akan diterapkan operasi max pooling\n",
    "        \n",
    "        Return:\n",
    "        pooled_images: Matriks Pooling Numpy Array 4D\n",
    "        \"\"\"\n",
    "        img_len, pooled_x, pooled_y, anchors = X.shape[0], X.shape[1] // pool_size[0], X.shape[2] // pool_size[1], X.shape[-1]\n",
    "        pooled_images = np.zeros((img_len, pooled_x, pooled_y, anchors))\n",
    "        \n",
    "        for idx in range(img_len):  \n",
    "            for i in range(0, pooled_x*pool_size[0], pool_size[0]):\n",
    "                for j in range(0, pooled_y*pool_size[1], pool_size[1]):\n",
    "                    sub_image = X[idx, i:i+pool_size[0], j:j+pool_size[1], :]\n",
    "                    pooled_images[idx, int(i/pool_size[0]), int(j/pool_size[1]), :] = np.max(sub_image, axis=(0, 1))\n",
    "        return pooled_images\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.A = self.pooling(X, self.pool_size)\n",
    "        return self.A\n",
    "    \n",
    "    def backpro(self, prev_layer, next_layer):\n",
    "        if next_layer.alias == \"flatten\":\n",
    "            self.dP = np.reshape(next_layer.dZ.T, newshape=self.A.shape)\n",
    "        else:\n",
    "            padded_size = next_layer.kernel_size[0] - 1\n",
    "            padded_dZ = np.pad(next_layer.dZ, ((0, 0), (padded_size, padded_size), (padded_size, padded_size), (0, 0)))\n",
    "            rotated_W = np.rot90(next_layer.W, axes=(1, 2), k=2)\n",
    "            self.dP = super().convolution(X=padded_dZ, W=rotated_W.swapaxes(0, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890a830c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten:\n",
    "    def __init__(self, input_shape=(None,)):\n",
    "        self.input_shape = (None,) + input_shape\n",
    "        self.alias = \"flatten\"\n",
    "        \n",
    "    def initialize_parameters(self, lr, beta1, beta2, epsilon):\n",
    "        # Ukuran Output\n",
    "        self.output_shape = (np.prod(self.input_shape[1:]), None)\n",
    "        # Jumlah Parameter\n",
    "        self.param_nums = 0\n",
    "        \n",
    "    def flatten(self, X):\n",
    "        \"\"\"\n",
    "        Mengubah matriks citra berdimensi 4 menjadi matriks berdimensi 2\n",
    "        \n",
    "        Parameter:\n",
    "        X: Matriks Input Numpy Array 4D berukuran (jumlah data, panjang pixel, lebar pixel, anchor)\n",
    "        \n",
    "        Return:\n",
    "        flat_images: Matriks Flatten Numpy Array 2D berukuran (jumlah fitur, jumlah data)\n",
    "        \"\"\"\n",
    "        flat_images = np.reshape(X, (X.shape[0], -1))\n",
    "        return flat_images.T\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.A = self.flatten(X)\n",
    "        return self.A\n",
    "    \n",
    "    def backpro(self, prev_layer, next_layer):\n",
    "        self.dZ = next_layer.W.T @ next_layer.dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f213f0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Activation):\n",
    "    def __init__(self, neuron, activation=\"relu\", input_shape=(None,)):\n",
    "        self.neuron = neuron\n",
    "        self.activation = activation\n",
    "        self.input_shape = (None,) + input_shape\n",
    "        self.alias = \"dense\"\n",
    "    \n",
    "    def initialize_parameters(self, lr, beta1, beta2, epsilon):\n",
    "        # Ukuran Output\n",
    "        self.output_shape = (self.neuron,) + (None,)\n",
    "        # Weight / Bobot\n",
    "        self.W = np.random.randn(self.neuron, self.input_shape[0]) * 0.001\n",
    "        self.adam_W = Adam(lr, beta1, beta2, epsilon)\n",
    "        # Bias\n",
    "        self.B = np.zeros((self.neuron, 1))\n",
    "        self.adam_B = Adam(lr, beta1, beta2, epsilon)\n",
    "        # Jumlah parameter\n",
    "        self.param_nums = np.prod(self.W.shape) + np.prod(self.B.shape)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        self.Z = self.W @ inputs + self.B\n",
    "        if self.activation == \"relu\":\n",
    "            activation_func = super().relu\n",
    "        elif self.activation == \"softmax\":\n",
    "            activation_func = super().softmax\n",
    "        self.A = activation_func(self.Z)\n",
    "        return self.A\n",
    "    \n",
    "    def backpro(self, prev_layer, next_layer):\n",
    "        img = self.A.shape[1]\n",
    "        if self.activation == \"relu\":\n",
    "            diff_Z = np.where(self.Z >= 0, 1, 0)\n",
    "            self.dZ = (next_layer.W.T @ next_layer.dZ) * diff_Z\n",
    "        elif self.activation == \"softmax\":\n",
    "            self.dZ = self.A - next_layer\n",
    "\n",
    "        self.dW = self.dZ @ prev_layer.A.T / img\n",
    "        self.dB = np.sum(self.dZ, axis=1, keepdims=True) / img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751a1e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential(Metrics):\n",
    "    def __init__(self, layers, lr, beta1, beta2, epsilon):\n",
    "        self.layers = layers\n",
    "        self.history = {\"acc\": [], \"loss\": [], \"val_acc\": [], \"val_loss\": []}\n",
    "        self.initialize_parameters(lr, beta1, beta2, epsilon)\n",
    "    \n",
    "    def initialize_parameters(self, lr, beta1, beta2, epsilon):\n",
    "        for idx, layer in enumerate(self.layers):\n",
    "            if idx != 0:\n",
    "                layer.input_shape = self.layers[idx-1].output_shape\n",
    "            layer.initialize_parameters(lr, beta1, beta2, epsilon)\n",
    "    \n",
    "    def summary(self):\n",
    "        \"\"\"\n",
    "        Menampilkan tabel yang berisi layer-layer yang dipakai dan banyak parameternya\n",
    "        \"\"\"\n",
    "        total_param_nums = 0\n",
    "        table = PrettyTable()\n",
    "        table.field_names = [\"Layer\", \"Output Shape\", \"Number of Parameters\"]\n",
    "        for layer in self.layers:\n",
    "            total_param_nums += layer.param_nums\n",
    "            table.add_row([layer.alias, layer.output_shape, layer.param_nums])\n",
    "        table.align = \"r\"\n",
    "        print(table,\"\\n\")\n",
    "        print(\"===========================================================\\n\")\n",
    "        print(f\"Number of Parameters: {total_param_nums}\")\n",
    "        \n",
    "    # Algoritma Feed Forward\n",
    "    def feed_forward(self, inputs):\n",
    "        for layer in self.layers:\n",
    "            inputs = layer.forward(inputs)\n",
    "        return inputs\n",
    "    \n",
    "    # Algoritma Backpropagation\n",
    "    def backpropagation(self, inputs, targets):\n",
    "        for idx, layer in enumerate(self.layers[::-1]):\n",
    "            prev_idx, next_idx = len(self.layers)-idx-2, len(self.layers)-idx\n",
    "            # Lapisan terakhir\n",
    "            if idx == 0:\n",
    "                next_layer = targets\n",
    "            else:\n",
    "                next_layer = self.layers[next_idx]\n",
    "            # Lapisan pertama\n",
    "            if prev_idx == -1:\n",
    "                prev_layer = inputs\n",
    "            else:\n",
    "                prev_layer = self.layers[prev_idx]\n",
    "            layer.backpro(prev_layer=prev_layer, next_layer=next_layer)\n",
    "            \n",
    "    # Adaptive Moment Estimation\n",
    "    def adam_optimize(self):\n",
    "        layers_with_parameters = [layer for layer in self.layers if (layer.alias == \"dense\" or layer.alias == \"conv_2d\")]\n",
    "        for layer in layers_with_parameters:\n",
    "            layer.W = layer.adam_W.update(layer.W, layer.dW)\n",
    "            layer.B = layer.adam_B.update(layer.B, layer.dB)\n",
    "    \n",
    "    # Melakukan pelatihan model\n",
    "    def fit(self, inputs, targets, epochs=10, train_batch_size=32, validation_data=None):\n",
    "        for epoch in range(epochs):\n",
    "            # Epoch i-th\n",
    "            print(f\"Epoch {epoch+1}:\")\n",
    "            batches = math.ceil(len(inputs)/train_batch_size)\n",
    "            acc = 0\n",
    "            loss = 0\n",
    "            for batch in range(batches):\n",
    "                if batch != range(batches)[-1]:\n",
    "                    inputs_batch = inputs[batch*train_batch_size:(batch+1)*train_batch_size]\n",
    "                    targets_batch = targets[:,batch*train_batch_size:(batch+1)*train_batch_size]\n",
    "                else:\n",
    "                    inputs_batch = inputs[batch*train_batch_size:]\n",
    "                    targets_batch = targets[:,batch*train_batch_size:]  \n",
    "                start = time.time()\n",
    "                \n",
    "                # Feed Forward Algorithm\n",
    "                train_y_pred = self.feed_forward(inputs_batch)\n",
    "                # Show metrics\n",
    "                acc += super().accuracy(train_y_pred, targets_batch)\n",
    "                loss += super().categorical_crossentropy(train_y_pred, targets_batch)\n",
    "                # Backpropagation Algorithm\n",
    "                self.backpropagation(inputs_batch, targets_batch)\n",
    "                # Optimizer: Adam\n",
    "                self.adam_optimize()\n",
    "                \n",
    "                end = time.time()\n",
    "                duration = round(end - start)\n",
    "                print(f\"Batch {batch+1}/{batches} | duration: {duration // 60}:{duration %  60} | accuracy: {acc/(batch+1)} | loss: {loss/(batch+1)}\")\n",
    "            \n",
    "            # Validation data\n",
    "            if validation_data and len(validation_data) == 2:\n",
    "                test_y_pred = self.feed_forward(validation_data[0])\n",
    "                val_acc = super().accuracy(test_y_pred, validation_data[1])\n",
    "                val_loss = super().categorical_crossentropy(test_y_pred, validation_data[1])\n",
    "                print(f\"Result:\\naccuracy: {acc/batches} | loss: {loss/batches} | val_accuracy: {val_acc} | val_loss: {val_loss}\")\n",
    "            print(\"\\n\")\n",
    "            \n",
    "            # Append the metric\n",
    "            self.history[\"acc\"].append(acc/batches)\n",
    "            self.history[\"loss\"].append(loss/batches)\n",
    "            self.history[\"val_acc\"].append(val_acc)\n",
    "            self.history[\"val_loss\"].append(val_loss)\n",
    "                \n",
    "    def save(self):\n",
    "        i = 1\n",
    "        layers_with_parameters = [layer for layer in self.layers if (layer.alias == \"dense\" or layer.alias == \"conv_2d\")]\n",
    "\n",
    "        for layer in layers_with_parameters:\n",
    "            np.save(f\"Saved Model/W{i}.npy\", layer.W)\n",
    "            np.save(f\"Saved Model/B{i}.npy\", layer.B)\n",
    "            i += 1\n",
    "            \n",
    "    def load(self):\n",
    "        i = 1\n",
    "        layers_with_parameters = [layer for layer in self.layers if (layer.alias == \"dense\" or layer.alias == \"conv_2d\")]\n",
    "\n",
    "        for layer in layers_with_parameters:\n",
    "            layer.W = np.load(f\"Saved Model/W{i}.npy\")\n",
    "            layer.B = np.load(f\"Saved Model/B{i}.npy\")\n",
    "            i += 1\n",
    "            \n",
    "    def predict(self, image_file):\n",
    "        # Load model\n",
    "        self.load()\n",
    "        \n",
    "        # Mengubah dimensi data citra yang akan diprediksi menjadi 4 dimensi\n",
    "        image = cv2.imread(image_file)\n",
    "        img_gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "        \n",
    "        ## Segmentasi gambar\n",
    "        # perform median blur\n",
    "        img_filt = cv2.medianBlur(img_gray, 5)\n",
    "\n",
    "        # perform Otsu's thresholding\n",
    "        _, thresh = cv2.threshold(img_filt, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "\n",
    "        # perform morphological opening to remove noise\n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (11,11))\n",
    "        opening = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "        # perform morphological closing to fill gaps\n",
    "        closing = cv2.morphologyEx(opening, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "        # find contours of breast region\n",
    "        contours, hierarchy = cv2.findContours(closing, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        breast_contour = max(contours, key=cv2.contourArea)\n",
    "\n",
    "        # create mask of breast region\n",
    "        mask = cv2.drawContours(img_filt.copy(), [breast_contour], 0, 255, -1)\n",
    "\n",
    "        # apply mask to original image\n",
    "        img_filt = cv2.resize(img_filt, model.layers[0].input_shape[1:3])\n",
    "        mask = cv2.resize(mask, model.layers[0].input_shape[1:3]) == 255\n",
    "        breast = img_filt * mask\n",
    "\n",
    "        # perform histogram equalization\n",
    "        img_eq = cv2.equalizeHist(breast)\n",
    "        \n",
    "        converted_image = np.expand_dims(img_eq, axis=(0, -1)) / 255\n",
    "        \n",
    "        # Feed Forward\n",
    "        y_pred = self.feed_forward(converted_image)\n",
    "        \n",
    "        # Menunjukkan hasil per layer\n",
    "        ## Data original\n",
    "        print(\"Original Image\")\n",
    "        plt.imshow(image, cmap=\"gray\")\n",
    "        plt.show()\n",
    "        print(\"---\"*12)\n",
    "        \n",
    "        ## Pemrosesan data per layer\n",
    "        inputs = converted_image.copy()\n",
    "        for layer in self.layers:\n",
    "            inputs = layer.forward(inputs)\n",
    "            if layer.alias in (\"conv_2d\", \"maxpool_2d\"):\n",
    "                print(layer.alias)\n",
    "                for filt in range(128):\n",
    "                    if np.sum(inputs[0, :, :, filt]) > 0:\n",
    "                        plt.imshow(inputs[0, :, :, filt], cmap=\"gray\")\n",
    "                        plt.title(f\"Filter {filt+1}\")\n",
    "                        plt.show()\n",
    "            print(\"---\"*12)\n",
    "        \n",
    "        # Memberi prediksi\n",
    "        plt.imshow(image, cmap=\"gray\")\n",
    "        plt.show()\n",
    "        print(f\"Hasil Prediksi Citra: {IDX_TO_PATHOLOGY[np.argmax(y_pred)]}\")\n",
    "        print(f\"Target Citra: {image_file.replace('.jpg', '')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1626673",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346233f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATHOLOGY_TO_IDX = {\"BENIGN\": 0, \"BENIGN_WITHOUT_CALLBACK\": 1, \"MALIGNANT\": 2}\n",
    "IDX_TO_PATHOLOGY = {idx: pathology for pathology, idx in PATHOLOGY_TO_IDX.items()}\n",
    "\n",
    "def data_preprocessing(csv_file, image_size=(100, 100)):\n",
    "    images_matrix = []\n",
    "    labels_matrix = []\n",
    "    \n",
    "    # Memuat data pada file csv\n",
    "    data = pd.read_csv(csv_file)\n",
    "    data = data[[\"pathology\", \"image file path\"]]\n",
    "    data.drop_duplicates(subset=\"image file path\", inplace=True)\n",
    "    \n",
    "    # Mengambil path subdir terakhir\n",
    "    for column in data.columns:\n",
    "        if column != \"pathology\":\n",
    "            data[column] = data[column].apply(lambda x: x.split(\"/\")[-2])\n",
    "    \n",
    "    # Pemrosesan data\n",
    "    for idx, row in data.iterrows():\n",
    "        # Citra original\n",
    "        img_path = f\"Data/jpeg/{row['image file path']}\"\n",
    "        img = cv2.imread(f\"{img_path}/{os.listdir(img_path)[0]}\")\n",
    "        img_gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        \n",
    "        ## Segmentasi gambar\n",
    "        # perform median blur\n",
    "        img_filt = cv2.medianBlur(img_gray, 5)\n",
    "\n",
    "        # perform Otsu's thresholding\n",
    "        _, thresh = cv2.threshold(img_filt, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "\n",
    "        # perform morphological opening to remove noise\n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (11,11))\n",
    "        opening = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "        # perform morphological closing to fill gaps\n",
    "        closing = cv2.morphologyEx(opening, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "        # find contours of breast region\n",
    "        contours, hierarchy = cv2.findContours(closing, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        breast_contour = max(contours, key=cv2.contourArea)\n",
    "\n",
    "        # create mask of breast region\n",
    "        mask = cv2.drawContours(img_filt.copy(), [breast_contour], 0, 255, -1)\n",
    "\n",
    "        # apply mask to original image\n",
    "        img_filt = cv2.resize(img_filt, image_size)\n",
    "        mask = cv2.resize(mask, image_size) == 255\n",
    "        breast = img_filt * mask\n",
    "\n",
    "        # perform histogram equalization\n",
    "        img_eq = cv2.equalizeHist(breast)\n",
    "        \n",
    "        images_matrix.append(breast)\n",
    "            \n",
    "        # Pemrosesan Target\n",
    "        labels_vector = np.zeros(len(PATHOLOGY_TO_IDX))\n",
    "        labels_vector[PATHOLOGY_TO_IDX[row['pathology']]] = 1\n",
    "        \n",
    "        labels_matrix.append(labels_vector)\n",
    "            \n",
    "    images_matrix = np.expand_dims(images_matrix, axis=-1) / 255\n",
    "    labels_matrix = np.array(labels_matrix)\n",
    "    \n",
    "    # Randomize the data\n",
    "    random_num = random.randint(0, 100)\n",
    "    X, Y = shuffle(images_matrix, labels_matrix, random_state=random_num)\n",
    "    return X, Y.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9694c57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_calc_images, train_calc_labels = data_preprocessing(\"Data/csv/calc_case_description_train_set.csv\", image_size=(50, 50))\n",
    "# train_mass_images, train_mass_labels = data_preprocessing(\"Data/csv/mass_case_description_train_set.csv\", image_size=(50, 50))\n",
    "# test_calc_images, test_calc_labels = data_preprocessing(\"Data/csv/calc_case_description_test_set.csv\", image_size=(50, 50))\n",
    "# test_mass_images, test_mass_labels = data_preprocessing(\"Data/csv/mass_case_description_test_set.csv\", image_size=(50, 50))\n",
    "\n",
    "# train_images, train_labels = np.concatenate((train_calc_images, train_mass_images), axis=0), np.concatenate((train_calc_labels, train_mass_labels), axis=1)\n",
    "# test_images, test_labels = np.concatenate((test_calc_images, test_mass_images), axis=0), np.concatenate((test_calc_labels, test_mass_labels), axis=1)\n",
    "\n",
    "# # train_images, train_labels = np.concatenate((train_images, test_images[:-100]), axis=0), np.concatenate((train_labels, test_labels[:, :-100]), axis=1)\n",
    "# # test_images, test_labels = test_images[-100:], test_labels[:, -100:]\n",
    "\n",
    "# test_images = np.concatenate((train_images[-350:], test_images[-295:]), axis=0)\n",
    "# test_labels = np.concatenate((train_labels[:, -350:], test_labels[:, -295:]), axis=1)\n",
    "\n",
    "# train_images, train_labels = shuffle(train_images, train_labels.T, random_state=42)\n",
    "# train_labels = train_labels.T\n",
    "\n",
    "# print(train_images.shape)\n",
    "# print(test_images.shape)\n",
    "# print(train_labels.shape)\n",
    "# print(test_labels.shape)\n",
    "\n",
    "# np.save(\"train_images.npy\", train_images)\n",
    "# np.save(\"test_images.npy\", test_images)\n",
    "# np.save(\"train_labels.npy\", train_labels)\n",
    "# np.save(\"test_labels.npy\", test_labels)\n",
    "\n",
    "train_images = np.load(\"train_images.npy\")\n",
    "train_labels = np.load(\"train_labels.npy\")\n",
    "test_images = np.load(\"test_images.npy\")\n",
    "test_labels = np.load(\"test_labels.npy\")\n",
    "\n",
    "print(f\"Ukuran data citra latih: {train_images.shape}\")\n",
    "print(f\"Ukuran data target pada citra latih: {train_labels.shape}\", \"\\n\")\n",
    "print(f\"Ukuran data citra uji secara utuh: {test_images.shape}\")\n",
    "print(f\"Ukuran data target pada citra uji secara utuh: {test_labels.shape}\")\n",
    "print(\"---\"*10, \"\\n\")\n",
    "\n",
    "new_test_images = test_images[-295:-195].copy()\n",
    "new_test_labels = test_labels[:, -295:-195].copy()\n",
    "\n",
    "print(f\"Ukuran data citra uji secara utuh: {new_test_images.shape}\")\n",
    "print(f\"Ukuran data target pada citra uji secara utuh: {new_test_labels.shape}\")\n",
    "print(\"---\"*10, \"\\n\")\n",
    "\n",
    "print(\"Ambil sembarang citra pada data latih\")\n",
    "random_num = random.randint(0, len(train_images) - 1)\n",
    "\n",
    "plt.imshow(train_images[random_num], cmap=\"gray\")\n",
    "plt.title(f\"{IDX_TO_PATHOLOGY[np.argmax(train_labels[:, random_num])]} No. {random_num}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72050b5",
   "metadata": {},
   "source": [
    "### Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca5469c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(128, (3,3), activation=\"relu\", input_shape=(50, 50, 1)),\n",
    "    MaxPooling((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(512, activation=\"relu\"),\n",
    "    Dense(3, activation=\"softmax\")\n",
    "], \n",
    "    lr=1e-3, beta1=0.9, beta2=0.999, epsilon=1e-7)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea143ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_imgs = [file for file in os.listdir() if \".jpg\" in file]\n",
    "demo_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fee88e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Choice: {0, 1, 2}\n",
    "choice = 2\n",
    "model.predict(demo_imgs[choice])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74b93b0",
   "metadata": {},
   "source": [
    "### 1e-3 dan 64 (Done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410899ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = Sequential([\n",
    "    Conv2D(128, (3,3), activation=\"relu\", input_shape=(50, 50, 1)),\n",
    "    MaxPooling((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(512, activation=\"relu\"),\n",
    "    Dense(3, activation=\"softmax\")\n",
    "], \n",
    "    lr=1e-3, beta1=0.9, beta2=0.999, epsilon=1e-7)\n",
    "\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366f20eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_1.fit(train_images, train_labels, train_batch_size=64, epochs=30, validation_data=(new_test_images, new_test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3e68ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Accuracy\n",
    "sns.lineplot(x=range(1, len(model_1.history[\"acc\"]) + 1), \n",
    "             y=model_1.history[\"acc\"],\n",
    "             label=\"Accuracy\"\n",
    "            )\n",
    "# Validation Accuracy\n",
    "sns.lineplot(x=range(1, len(model_1.history[\"val_acc\"]) + 1), \n",
    "             y=model_1.history[\"val_acc\"],\n",
    "             label=\"Validation Accuracy\"\n",
    "            )\n",
    "\n",
    "# More information\n",
    "plt.title(\"Model Accuracy\", fontsize=14)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28bedee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "sns.lineplot(x=range(1, len(model_1.history[\"loss\"]) + 1), \n",
    "             y=model_1.history[\"loss\"],\n",
    "             label=\"Loss\"\n",
    "            )\n",
    "\n",
    "# Validation Accuracy\n",
    "sns.lineplot(x=range(1, len(model_1.history[\"val_loss\"]) + 1), \n",
    "             y=model_1.history[\"val_loss\"],\n",
    "             label=\"Validation Loss\"\n",
    "            )\n",
    "\n",
    "# More information\n",
    "plt.title(\"Model Loss\", fontsize=14)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880587f6",
   "metadata": {},
   "source": [
    "### 1e-3 dan 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9765db",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = Sequential([\n",
    "    Conv2D(128, (3,3), activation=\"relu\", input_shape=(50, 50, 1)),\n",
    "    MaxPooling((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(512, activation=\"relu\"),\n",
    "    Dense(3, activation=\"softmax\")\n",
    "], \n",
    "    lr=1e-3, beta1=0.9, beta2=0.999, epsilon=1e-7)\n",
    "\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967fad10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_2.fit(train_images, train_labels, train_batch_size=32, epochs=30, validation_data=(new_test_images, new_test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca76cf2",
   "metadata": {},
   "source": [
    "### 1e-3 dan 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb0d983",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3 = Sequential([\n",
    "    Conv2D(128, (3,3), activation=\"relu\", input_shape=(50, 50, 1)),\n",
    "    MaxPooling((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(512, activation=\"relu\"),\n",
    "    Dense(3, activation=\"softmax\")\n",
    "], \n",
    "    lr=1e-3, beta1=0.9, beta2=0.999, epsilon=1e-7)\n",
    "\n",
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c30c90b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_3.fit(train_images, train_labels, train_batch_size=128, epochs=30, validation_data=(new_test_images, new_test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f063ba",
   "metadata": {},
   "source": [
    "### 1e-2 dan 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d68236",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4 = Sequential([\n",
    "    Conv2D(128, (3,3), activation=\"relu\", input_shape=(50, 50, 1)),\n",
    "    MaxPooling((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(512, activation=\"relu\"),\n",
    "    Dense(3, activation=\"softmax\")\n",
    "], \n",
    "    lr=1e-2, beta1=0.9, beta2=0.999, epsilon=1e-7)\n",
    "\n",
    "model_4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ed17a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_4.fit(train_images, train_labels, train_batch_size=64, epochs=30, validation_data=(new_test_images, new_test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e856f2ae",
   "metadata": {},
   "source": [
    "### 1e-2 dan 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674ea468",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_5 = Sequential([\n",
    "    Conv2D(128, (3,3), activation=\"relu\", input_shape=(50, 50, 1)),\n",
    "    MaxPooling((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(512, activation=\"relu\"),\n",
    "    Dense(3, activation=\"softmax\")\n",
    "], \n",
    "    lr=1e-2, beta1=0.9, beta2=0.999, epsilon=1e-7)\n",
    "\n",
    "model_5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ecb2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_5.fit(train_images, train_labels, train_batch_size=32, epochs=30, validation_data=(new_test_images, new_test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0713e5f",
   "metadata": {},
   "source": [
    "### 1e-2 dan 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5968fc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_6 = Sequential([\n",
    "    Conv2D(128, (3,3), activation=\"relu\", input_shape=(50, 50, 1)),\n",
    "    MaxPooling((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(512, activation=\"relu\"),\n",
    "    Dense(3, activation=\"softmax\")\n",
    "], \n",
    "    lr=1e-2, beta1=0.9, beta2=0.999, epsilon=1e-7)\n",
    "\n",
    "model_6.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea9aa8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_6.fit(train_images, train_labels, train_batch_size=128, epochs=30, validation_data=(new_test_images, new_test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ebde0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
